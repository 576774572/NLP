{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd882e7d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers==4.25.1\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting TorchCRF==1.1.0\n",
      "  Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting fugashi\n",
      "  Downloading fugashi-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.3/613.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ipadic\n",
      "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mecab-python3\n",
      "  Downloading mecab_python3-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.5/581.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting unidic-lite\n",
      "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jaconv\n",
      "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.25.1) (4.63.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.25.1) (2022.10.31)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.25.1) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.25.1) (1.23.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.25.1) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.25.1) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.25.1) (21.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from TorchCRF==1.1.0) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.25.1) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.25.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.25.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.25.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.25.1) (1.26.8)\n",
      "Building wheels for collected packages: ipadic, unidic-lite, jaconv\n",
      "  Building wheel for ipadic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556703 sha256=acb8cb32ff201d15727d08141b3d7aa9ca2719ed52e14c00aa1ea98c4287bf41\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4a/fb/28/21aa4bc5d871bbe3797f8372833fe4c589e1984c7bda05db16\n",
      "  Building wheel for unidic-lite (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=dbe5b020bc734f45851944e356bc24c9898c08efa1fb56d056aad0005e69bab7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/75/2f/033d31f7054809b88a2670cc754e1f378687389c0964c6ea1e\n",
      "  Building wheel for jaconv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16417 sha256=7d39db1dfc199cde66f87efa1f1cf733309a9435960463852e61c7863d2d62cf\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f5/03/b1/090bfac2a7f91870077434d8abf61a7af8a02d3bbbfe1ce08e\n",
      "Successfully built ipadic unidic-lite jaconv\n",
      "Installing collected packages: unidic-lite, tokenizers, mecab-python3, jaconv, ipadic, fugashi, TorchCRF, huggingface-hub, transformers\n",
      "Successfully installed TorchCRF-1.1.0 fugashi-1.2.1 huggingface-hub-0.13.2 ipadic-1.0.0 jaconv-0.3.4 mecab-python3-1.0.6 tokenizers-0.13.2 transformers-4.25.1 unidic-lite-1.0.8\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.25.1 TorchCRF==1.1.0 fugashi ipadic mecab-python3 unidic-lite jaconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539315dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.15\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58424af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id 50892\n",
      "bert_model_path: cl-tohoku/bert-large-japanese\n",
      "tokenizer_path: cl-tohoku/bert-large-japanese\n",
      "output_path: output/run_2023.3.14_50892\n",
      "input_data_path: input_data\n",
      "create_labels_input_data: df_row.pkl\n",
      "create_labels_output_data: df_row_token_classification_dataset.pkl\n",
      "input_train_data: df_row_token_classification_dataset.pkl\n",
      "device: cuda\n",
      "device_count: 4\n",
      "label_num: 3\n",
      "batch_size: 128\n",
      "epoch_num: 1\n",
      "learning_rate: 3e-05\n",
      "weight_decay: 0.01\n",
      "clip_grad: 5\n",
      "last_hidden_state_dropout_prob: 0.2\n",
      "hidden_size: 1024\n",
      "now: 2023-03-14 08:06:20.853676\n",
      "run_id: 50892\n",
      "input_traindata_path: input_data/run_2023.3.14_50892\n",
      "output_model_path: output/run_2023.3.14_50892/model\n",
      "-----------split data--------------\n",
      "traindata: 798 testdata: 200\n",
      "traindata: 798 testdata: 200\n",
      "traindata: 798 testdata: 200\n",
      "traindata: 799 testdata: 199\n",
      "traindata: 799 testdata: 199\n",
      "--------Start Training--------\n",
      "cross : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 798 testdata: 200\n",
      "epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:23<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 15.556692464011055\n",
      "Validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.95      0.97      0.96      4377\n",
      "           B       0.72      0.74      0.73       344\n",
      "           I       0.87      0.78      0.82       848\n",
      "\n",
      "    accuracy                           0.92      5569\n",
      "   macro avg       0.85      0.83      0.84      5569\n",
      "weighted avg       0.92      0.92      0.92      5569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.95      0.97      0.96      4377\n",
      "         B+I       0.86      0.80      0.83      1192\n",
      "\n",
      "    accuracy                           0.93      5569\n",
      "   macro avg       0.91      0.88      0.89      5569\n",
      "weighted avg       0.93      0.93      0.93      5569\n",
      "\n",
      "--------Save best model--------\n",
      "Training Finished\n",
      "cross : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 798 testdata: 200\n",
      "epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 16.376482009887695\n",
      "Validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.95      0.96      0.96      4231\n",
      "           B       0.69      0.68      0.69       330\n",
      "           I       0.82      0.78      0.80       811\n",
      "\n",
      "    accuracy                           0.92      5372\n",
      "   macro avg       0.82      0.81      0.81      5372\n",
      "weighted avg       0.91      0.92      0.92      5372\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.95      0.96      0.96      4231\n",
      "         B+I       0.85      0.81      0.83      1141\n",
      "\n",
      "    accuracy                           0.93      5372\n",
      "   macro avg       0.90      0.89      0.89      5372\n",
      "weighted avg       0.93      0.93      0.93      5372\n",
      "\n",
      "--------Save best model--------\n",
      "Training Finished\n",
      "cross : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 798 testdata: 200\n",
      "epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 16.83804770878383\n",
      "Validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.93      0.97      0.95      4473\n",
      "           B       0.70      0.70      0.70       338\n",
      "           I       0.86      0.70      0.77       865\n",
      "\n",
      "    accuracy                           0.91      5676\n",
      "   macro avg       0.83      0.79      0.81      5676\n",
      "weighted avg       0.91      0.91      0.91      5676\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.93      0.97      0.95      4473\n",
      "         B+I       0.86      0.74      0.80      1203\n",
      "\n",
      "    accuracy                           0.92      5676\n",
      "   macro avg       0.90      0.85      0.87      5676\n",
      "weighted avg       0.92      0.92      0.92      5676\n",
      "\n",
      "--------Save best model--------\n",
      "Training Finished\n",
      "cross : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 799 testdata: 199\n",
      "epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 17.752125671931676\n",
      "Validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.92      0.97      0.95      4496\n",
      "           B       0.73      0.64      0.68       355\n",
      "           I       0.85      0.66      0.74       851\n",
      "\n",
      "    accuracy                           0.90      5702\n",
      "   macro avg       0.84      0.76      0.79      5702\n",
      "weighted avg       0.90      0.90      0.90      5702\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.92      0.97      0.95      4496\n",
      "         B+I       0.87      0.69      0.77      1206\n",
      "\n",
      "    accuracy                           0.91      5702\n",
      "   macro avg       0.89      0.83      0.86      5702\n",
      "weighted avg       0.91      0.91      0.91      5702\n",
      "\n",
      "--------Save best model--------\n",
      "Training Finished\n",
      "cross : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata: 799 testdata: 199\n",
      "epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 17.376716818128312\n",
      "Validate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.97      0.97      0.97      4228\n",
      "           B       0.78      0.78      0.78       292\n",
      "           I       0.85      0.85      0.85       682\n",
      "\n",
      "    accuracy                           0.94      5202\n",
      "   macro avg       0.87      0.87      0.87      5202\n",
      "weighted avg       0.94      0.94      0.94      5202\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.97      0.97      0.97      4228\n",
      "         B+I       0.86      0.86      0.86       974\n",
      "\n",
      "    accuracy                           0.95      5202\n",
      "   macro avg       0.91      0.91      0.91      5202\n",
      "weighted avg       0.95      0.95      0.95      5202\n",
      "\n",
      "--------Save best model--------\n",
      "Training Finished\n",
      "Cross_validation_result_labels saving Finished\n",
      "--------Start Evaluation--------\n",
      "cross : 1\n",
      "cross_ 1  pred_pick complete\n",
      "cross_ 1  result_pick complete\n",
      "cross_ 1  miss_pick complete\n",
      "cross_ 1  extra_pick complete\n",
      "cross : 2\n",
      "cross_ 2  pred_pick complete\n",
      "cross_ 2  result_pick complete\n",
      "cross_ 2  miss_pick complete\n",
      "cross_ 2  extra_pick complete\n",
      "cross : 3\n",
      "cross_ 3  pred_pick complete\n",
      "cross_ 3  result_pick complete\n",
      "cross_ 3  miss_pick complete\n",
      "cross_ 3  extra_pick complete\n",
      "cross : 4\n",
      "cross_ 4  pred_pick complete\n",
      "cross_ 4  result_pick complete\n",
      "cross_ 4  miss_pick complete\n",
      "cross_ 4  extra_pick complete\n",
      "cross : 5\n",
      "cross_ 5  pred_pick complete\n",
      "cross_ 5  result_pick complete\n",
      "cross_ 5  miss_pick complete\n",
      "cross_ 5  extra_pick complete\n",
      "\n",
      "AE recall: 0.8637274549098196\n",
      "AE precision: 0.9410480349344978\n",
      "Cross_validation_test_result_df saving Finished\n",
      "Evaluation Finished\n"
     ]
    }
   ],
   "source": [
    "from config import get_config\n",
    "from labels_create import labels_create\n",
    "from utils import create_log\n",
    "from train import train\n",
    "from evaluation import evaluation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Config\n",
    "    config=get_config()\n",
    "\n",
    "    # Create Dataset Labels\n",
    "    labels_create(config)  \n",
    "\n",
    "    # Create log & output_path\n",
    "    config=create_log(config)\n",
    "    \n",
    "    cross_validation=True # True ,False\n",
    "    \n",
    "    # Train\n",
    "    train(config,cross_validation=cross_validation) \n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation(config,cross_validation=cross_validation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d7333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
